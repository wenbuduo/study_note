## 文本数据
- 在处理文本之前，必须对文本进行切分词（tokenzie）操作，也就是将文本切分为单元（token）。在最简单的情形下，一个 token 就是一个单词。但直接按单词切分可能会损失一些信息，比如「Santa Barbara」应该是一个整体，却被切分为 2 个 token。现成的分词器（tokenizer）会考虑语言的特性，但也会出错，特别是当你处理特定来源的文本时（报纸、俚语、误拼、笔误）。
- 接下来需要正则化数据。对文本而言，这涉及词干提取（stemming）和词形还原（lemmatization）方法。这两个方法是词形规范化的两类重要方式，都能够达到有效归并词形的目的，二者既有联系也有区别。两者的区别可以参考《Introduction to Information Retrieval》一书的 [<i class="fa fa-external-link-square" aria-hidden="true"> Stemming and lemmatization</i>](http://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html) 一节。
- 当文档被转换为单词序列之后，就可以用向量表示它。最简单的方法是词袋（Bag of Words）模型：创建一个长度等于字典的向量，计算每个单词出现在文本中的次数，然后将次数放入向量对应的位置中。
```
import numpy as np
import pandas as pd
texts = ['i have a cat',
         'you have a dog',
         'you and i have a cat and a dog']

vocabulary = list(enumerate(set([word for sentence
                                 in texts for word in sentence.split()])))
print('Vocabulary:', vocabulary)


def vectorize(text):
    vector = np.zeros(len(vocabulary))
    for i, word in vocabulary:
        num = 0
        for w in text:
            if w == word:
                num += 1
        if num:
            vector[i] = num
    return vector


print('Vectors:')
for sentence in texts:
    print(vectorize(sentence.split()))
```
Vocabulary: [(0, 'and'), (1, 'dog'), (2, 'you'), (3, 'cat'), (4, 'i'), (5, 'have'), (6, 'a')]
Vectors:
[0. 0. 0. 1. 1. 1. 1.]
[0. 1. 1. 0. 0. 1. 1.]
[2. 1. 1. 1. 1. 1. 2.]
当使用词袋模型时，文本中的单词顺序信息会丢失，这意味着向量化之后，“i have no cows”（我没有牛）和“no, i have cows”（没，我有牛）会变得一样，尽管事实上它们的意思截然相反。为了避免这个问题，可以转而使用 N-Gram 模型。下面载入相关库，建立 N-Gram 模型。
```
from sklearn.feature_extraction.text import CountVectorizer

vect = CountVectorizer(ngram_range=(1, 1))
vect.fit_transform(['no i have cows', 'i have no cows']).toarray()
vect.vocabulary
#输出为：array([[1, 1, 1],[1, 1, 1]])
#{'no': 2, 'have': 1, 'cows': 0}
#改变参数，再次建立 N-Gram 模型。
vect = CountVectorizer(ngram_range=(1, 2))
vect.fit_transform(['no i have cows', 'i have no cows']).toarray()
#array([[1, 1, 1, 0, 1, 0, 1],[1, 1, 0, 1, 1, 1, 0]])
vect.vocabulary
%% {'no': 4,
 'have': 1,
 'cows': 0,
 'no have': 6,
 'have cows': 2,
 'have no': 3,
 'no cows': 5} %%
```
除了基于单词生成 N-Gram 模型，在某些情形下，可以基于字符生成 N-Gram 模型，以考虑相关单词的相似性或笔误，下面基于字符生成 N-Gram 模型，并查看它们之间的欧氏距离。
```
from scipy.spatial.distance import euclidean
from sklearn.feature_extraction.text import CountVectorizer

vect = CountVectorizer(ngram_range=(3, 3), analyzer='char_wb')

n1, n2, n3, n4 = vect.fit_transform(
    ['andersen', 'petersen', 'petrov', 'smith']).toarray()

euclidean(n1, n2), euclidean(n2, n3), euclidean(n3, n4)
```
![[Pasted image 20241025155949.png]]
![[Pasted image 20241025160000.png]]
值得注意的是，Word2Vec 模型并不理解单词的意思，只是尝试将用于相同的上下文的单词向量放在相近的位置。这样的模型需要在非常大的数据集上训练，使向量坐标能够捕捉语义
##  图像处理
![[Pasted image 20241025161602.png]]![[Pasted image 20241025160827.png]]
代码示例：
```
#重新调整尺寸
img = img.resize((224, 224))
#通过 `expand_dims()` 方法添加一个额外的维度，以适配网络模型的输入格式，其输入格式为张量形状（batch_size, width, height, n_channels）。
x = image.img_to_array(img)
x = np.expand_dims(x, axis=0)
x = preprocess_input(x)
features = resnet.predict(x)
```
## 地理空间
![[Pasted image 20241025161741.png]]
![[Pasted image 20241025161801.png]]

## 时间和日期数据
![[Pasted image 20241025161922.png]]
![[Pasted image 20241025161931.png]]
![[Pasted image 20241025162005.png]]
上面结果可知，它们之间的距离各不相同，距离信息被保留
## Web数据
Web 数据通常有用户的 User Agent 信息，这个信息非常重要，首先，从中可以提取操作系统信息。其次，可以据此创建 「is_mobile」（是否是移动端）特征。最后，可以通过它查看到浏览器类别。下面看看是如何操作的。
```
!pip install user_agents  # 安装所需模块
import user_agents

ua = 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Ubuntu Chromium/56.0.2924.76 Chrome/56.0.2924.76 Safari/537.36'
ua = user_agents.parse(ua)
print('Is a bot? ', ua.is_bot)
print('Is mobile? ', ua.is_mobile)
print('Is PC? ', ua.is_pc)
print('OS Family: ', ua.os.family)
print('OS Version: ', ua.os.version)
print('Browser Family: ', ua.browser.family)
print('Browser Version: ', ua.browser.version)
```
除了操作系统和浏览器外，你还可以查看 referrer、Accept-Language 和其他元信息。另一个有用的特征是 IP 地址，基于该数据可以提取出国家，乃至城市、网络运营商、连接类型（是否为移动网络）等信息。当然，该特征可能由于代理和数据库过期导致提取出的信息不准确，出现噪声。网络管理专家可能会尝试提取出更专业的信息，比如是否使用 VPN。另外，IP 地址数据和 Accept-Languag 是一对优秀的组合，它们可以互相印证：如果用户的 IP 地址显示在智利，而浏览器的本地化设为 ru_RU（俄罗斯），那么该用户的所在地就不清楚了，我们需要查看对应的特征栏 「is_traveler_or_proxy_user」（是旅行者还是代理用户）。

