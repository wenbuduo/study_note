## 集成
某种意义上，孔多塞陪审团定理形象的描述了上面提到的集成概念。该定理的内容为：如果评审团的每个成员做出独立判断，并且每个陪审员做出正确决策的概率高于 0.5，那么整个评审团做出正确的总体决策的概率随着陪审员数量的增加而增加，并趋向于一。另一方面，如果每个陪审员判断正确的概率小于 0.5，那么整个陪审团做出正确的总体决策的概率随着陪审员数量的增加而减少，并趋向于零。
$$  \mu = \sum_{i=m}^{N}{N\choose i}p^i(1-p)^{N-i} $$
- $N$ 为陪审员总数。
- $m$ 是构成多数的最小值，即 $m=\operatorname{floor}(N / 2)+1$。
- $ {N \choose i}$ 是组合数。
- $ p$ 为评审员做出正确决策的概率。
- $ \mu$ 是整个评审团做出正确决策的概率。
- 让我们看看另一个集成的例子：群体的智慧。1906 年，Francis Galton 访问了普利茅斯的一个农村集市，在那里他看到一个比赛。800 个参与者尝试估计一头屠宰的牛的重量。所有参与者的预测的平均值为 1197 磅，与牛的真实重量 1198 磅十分接近。
## Bootstrapping

Leo Breiman 于 1994 年提出的 Bagging（又称 Bootstrap Aggregation，引导聚集）是最基本的集成技术之一。Bagging 基于统计学中的 Bootstraping（自助法），该方法令复杂模型的统计评估变得更加可行。
Bootstrap 方法的流程如下：假设有尺寸为 N 的样本 X，从该样本中有放回地随机抽取 N 个样本，以创建一个新样本。换句话说，从尺寸为 N 的原样本中随机选择一个元素，并重复此过程 N 次。选中所有元素的可能性是一样的，因此每个元素被抽中的概率均为 $\frac{1}{N}$。

## Bagging
假设我们有一个训练集 X。我们使用 Bootstrap 生成样本 $ X_1, \dots, X_M$。现在，我们在每个 Bootstrap 样本上分别训练分类器 $ a_i(x)$，最终的分类器将对所有这些单独的分类器的输出取均值。在分类情形下，这种技术即投票（voting）：$ $$a(x) = \frac{1}{M}\sum_{i = 1}^M a_i(x).$$![[Pasted image 20241025112445.png]]

在回归问题中，通过对回归结果取均值，Bagging 将均方误差降至 $\frac{1}{M}$（M 为回归器数量）。

## 袋外误差（OOB Error）
随机森林不需要使用交叉验证或留置样本，因为它内置了误差估计。随机森林中的决策树基于原始数据集中不同的 Bootstrap 样本构建。对第 K 棵树而言，其特定 Bootstrap 样本大约留置了 37% 的输入。
这很容易证明。设数据集中有 $\ell$ 个样本。在每一步，每个数据点最终出现在有放回的 Bootstrap 样本中的概率均为 $\frac{1}{\ell}$。Bootstrap 样本最终不包含特定数据集元素的概率（即，该元素在 $\ell$  次抽取中都没抽中）等于 $(1 - \frac{1}{\ell})^\ell$。当 $\ell \rightarrow +\infty$ 时，这一概率等于  $\frac{1}{e}$。因此，选中某一特定样本的概率 $\approx  1 - \frac{1}{e} \approx 63\%$。
总结一下，每个基础算法在约 63% 的原始样本上训练。该算法可以在剩下的约 37% 的样本上验证。袋外估计仅仅是基础算法在 37% 样本上的平均估计。