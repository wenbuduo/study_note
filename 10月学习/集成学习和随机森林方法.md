## 集成
某种意义上，孔多塞陪审团定理形象的描述了上面提到的集成概念。该定理的内容为：如果评审团的每个成员做出独立判断，并且每个陪审员做出正确决策的概率高于 0.5，那么整个评审团做出正确的总体决策的概率随着陪审员数量的增加而增加，并趋向于一。另一方面，如果每个陪审员判断正确的概率小于 0.5，那么整个陪审团做出正确的总体决策的概率随着陪审员数量的增加而减少，并趋向于零。
$$  \mu = \sum_{i=m}^{N}{N\choose i}p^i(1-p)^{N-i} $$
- $N$ 为陪审员总数。
- $m$ 是构成多数的最小值，即 $m=\operatorname{floor}(N / 2)+1$。
- $ {N \choose i}$ 是组合数。
- $ p$ 为评审员做出正确决策的概率。
- $ \mu$ 是整个评审团做出正确决策的概率。
- 让我们看看另一个集成的例子：群体的智慧。1906 年，Francis Galton 访问了普利茅斯的一个农村集市，在那里他看到一个比赛。800 个参与者尝试估计一头屠宰的牛的重量。所有参与者的预测的平均值为 1197 磅，与牛的真实重量 1198 磅十分接近。
## Bootstrapping

Leo Breiman 于 1994 年提出的 Bagging（又称 Bootstrap Aggregation，引导聚集）是最基本的集成技术之一。Bagging 基于统计学中的 Bootstraping（自助法），该方法令复杂模型的统计评估变得更加可行。
Bootstrap 方法的流程如下：假设有尺寸为 N 的样本 X，从该样本中有放回地随机抽取 N 个样本，以创建一个新样本。换句话说，从尺寸为 N 的原样本中随机选择一个元素，并重复此过程 N 次。选中所有元素的可能性是一样的，因此每个元素被抽中的概率均为 $\frac{1}{N}$。

## Bagging
假设我们有一个训练集 X。我们使用 Bootstrap 生成样本 $ X_1, \dots, X_M$。现在，我们在每个 Bootstrap 样本上分别训练分类器 $ a_i(x)$，最终的分类器将对所有这些单独的分类器的输出取均值。在分类情形下，这种技术即投票（voting）：$ $$a(x) = \frac{1}{M}\sum_{i = 1}^M a_i(x).$$![[Pasted image 20241025112445.png]]

在回归问题中，通过对回归结果取均值，Bagging 将均方误差降至 $\frac{1}{M}$（M 为回归器数量）。

## 袋外误差（OOB Error）
随机森林不需要使用交叉验证或留置样本，因为它内置了误差估计。随机森林中的决策树基于原始数据集中不同的 Bootstrap 样本构建。对第 K 棵树而言，其特定 Bootstrap 样本大约留置了 37% 的输入。
这很容易证明。设数据集中有 $\ell$ 个样本。在每一步，每个数据点最终出现在有放回的 Bootstrap 样本中的概率均为 $\frac{1}{\ell}$。Bootstrap 样本最终不包含特定数据集元素的概率（即，该元素在 $\ell$  次抽取中都没抽中）等于 $(1 - \frac{1}{\ell})^\ell$。当 $\ell \rightarrow +\infty$ 时，这一概率等于  $\frac{1}{e}$。因此，选中某一特定样本的概率 $\approx  1 - \frac{1}{e} \approx 63\%$。
总结一下，每个基础算法在约 63% 的原始样本上训练。该算法可以在剩下的约 37% 的样本上验证。袋外估计仅仅是基础算法在 37% 样本上的平均估计。

## 随机森林
在 Bagging 中，主要使用的基础分类器为决策树，因为决策树相当复杂，并能在任何样本上达到零分类误差。随机子空间方法可以降低树的相关性，从而避免过拟合。基于 Bagging，基础算法会在不同的原始特征集的随机子集上训练。
- 设样本数等于 n，特征维度数等于 d。
- 选择模型的数目 M。
- 对于每个模型 m，选择特征数 dm<d。所有模型使用相同的 dm 值。
- 对每个模型 m，通过在整个 d 特征集合上随机选择 dm 个特征创建一个训练集。
- 训练每个模型。
- 通过组合 M 个模型的结果，将生成的整体模型应用于新数据中，应用时可以基于多数投票算法（majority voting）或后验概率聚合（aggregation of the posterior probabilities）。

构建 N 棵树的随机森林算法如下：
- 对每个 $k = 1, \dots, N$，生成 Bootstrap 样本 $X_k$。
- 在样本 $X_k$ 上创建一棵决策树 $b_k$：
  - 根据给定的标准选择最佳的特征维度。根据该特征分割样本以创建树的新层。重复这一流程，直到样本用尽。
  - 创建树，直到任何叶节点包含的实例不超过 $n_\text{min}$ 个，或者达到特定深度。
  - 每次分割，首先从 $d$ 个原始特征中随机选择 $m$ 个特征，接着只在该子集上搜索最佳分割。
最终分类器定义为：
$$ a(x) = \frac{1}{N}\sum_{k = 1}^N b_k(x)$$
scikit-learn 库提供了 `BaggingRegressor` 和 `BaggingClassifier` 类，其中在创建新模型时需要注意的一些参数如下：
- n_estimators 是随机森林中树的数量；
- criterion 是衡量分割质量的函数；
- max_features 是查找最佳分割时考虑的特征数；
- min_samples_leaf 是叶节点的最小样本数；
- max_depth 是树的最大深度。

## 随机森林，Bagging,决策树效果比较：
```
from sklearn.model_selection import train_test_split
from sklearn.datasets import make_circles
from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier
from sklearn.ensemble import BaggingClassifier, BaggingRegressor
from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier
import seaborn as sns
from matplotlib import pyplot as plt

# Disable warnings in Anaconda
import warnings
import numpy as np
warnings.filterwarnings('ignore')

%matplotlib inline
plt.style.use('ggplot')
plt.rcParams['figure.figsize'] = 10, 6


n_train = 150
n_test = 1000
noise = 0.1
def f(x):
    x = x.ravel()
    return np.exp(-x ** 2) + 1.5 * np.exp(-(x - 2) ** 2)


def generate(n_samples, noise):
    X = np.random.rand(n_samples) * 10 - 5
    X = np.sort(X).ravel()
    y = np.exp(-X ** 2) + 1.5 * np.exp(-(X - 2) ** 2)\
        + np.random.normal(0.0, noise, n_samples)
    X = X.reshape((n_samples, 1))

    return X, y


X_train, y_train = generate(n_samples=n_train, noise=noise)
X_test, y_test = generate(n_samples=n_test, noise=noise)

#使用单颗决策树回归
dtree = DecisionTreeRegressor().fit(X_train, y_train)
d_predict = dtree.predict(X_test)

plt.figure(figsize=(10, 6))
plt.plot(X_test, f(X_test), "b")
plt.scatter(X_train, y_train, c="b", s=20)
plt.plot(X_test, d_predict, "g", lw=2)
plt.xlim([-5, 5])
plt.title("Decision tree, MSE = %.2f"
% np.sum((y_test - d_predict) ** 2))

#使用 Bagging 决策树回归。
bdt = BaggingRegressor(DecisionTreeRegressor()).fit(X_train, y_train)
bdt_predict = bdt.predict(X_test)

plt.figure(figsize=(10, 6))
plt.plot(X_test, f(X_test), "b")
plt.scatter(X_train, y_train, c="b", s=20)
plt.plot(X_test, bdt_predict, "y", lw=2)
plt.xlim([-5, 5])
plt.title("Bagging for decision trees, MSE = %.2f" %
np.sum((y_test - bdt_predict) ** 2))

#使用随机森林
rf = RandomForestRegressor(n_estimators=10).fit(X_train, y_train)
rf_predict = rf.predict(X_test)

plt.figure(figsize=(10, 6))
plt.plot(X_test, f(X_test), "b")
plt.scatter(X_train, y_train, c="b", s=20)
plt.plot(X_test, rf_predict, "r", lw=2)
plt.xlim([-5, 5])
plt.title("Random forest, MSE = %.2f" % np.sum((y_test - rf_predict) ** 2))
```
从上面 3 张图像和 MSE 值可以看到，10 树随机森林比单棵决策树和 10 树 Bagging 的表现要好，当然随机森林在这个例子上不稳定，多次运行结果表明随机森林和 Bagging 决策树互有胜负。随机森林和 Bagging 的主要差别在于，前者分割的最佳特征是从一个随机特征子空间中选取的，而后者在分割时将考虑所有特征。

接下来，我们将查看随机森林和 Bagging 在分类问题上的表现。
```
np.random.seed(42)
X, y = make_circles(n_samples=500, factor=0.1, noise=0.35, random_state=42)
X_train_circles, X_test_circles, y_train_circles, y_test_circles = train_test_split(
    X, y, test_size=0.2)

dtree = DecisionTreeClassifier(random_state=42)
dtree.fit(X_train_circles, y_train_circles)

x_range = np.linspace(X.min(), X.max(), 100)
xx1, xx2 = np.meshgrid(x_range, x_range)
y_hat = dtree.predict(np.c_[xx1.ravel(), xx2.ravel()])
y_hat = y_hat.reshape(xx1.shape)
plt.contourf(xx1, xx2, y_hat, alpha=0.2)
plt.scatter(X[:, 0], X[:, 1], c=y, cmap='autumn')
plt.title("Decision tree")
plt.show()

b_dtree = BaggingClassifier(
    DecisionTreeClassifier(), n_estimators=300, random_state=42)
b_dtree.fit(X_train_circles, y_train_circles)

x_range = np.linspace(X.min(), X.max(), 100)
xx1, xx2 = np.meshgrid(x_range, x_range)
y_hat = b_dtree.predict(np.c_[xx1.ravel(), xx2.ravel()])
y_hat = y_hat.reshape(xx1.shape)
plt.contourf(xx1, xx2, y_hat, alpha=0.2)
plt.scatter(X[:, 0], X[:, 1], c=y, cmap='autumn')
plt.title("Bagging (decision trees)")
plt.show()

rf = RandomForestClassifier(n_estimators=300, random_state=42)
rf.fit(X_train_circles, y_train_circles)

x_range = np.linspace(X.min(), X.max(), 100)
xx1, xx2 = np.meshgrid(x_range, x_range)
y_hat = rf.predict(np.c_[xx1.ravel(), xx2.ravel()])
y_hat = y_hat.reshape(xx1.shape)
plt.contourf(xx1, xx2, y_hat, alpha=0.2)
plt.scatter(X[:, 0], X[:, 1], c=y, cmap='autumn')
plt.title("Random forest")
plt.show()
```
![[Pasted image 20241025145418.png]]![[Pasted image 20241025145432.png]]![[Pasted image 20241025145550.png]]
上图显示了决策树形成的边界凹凸不平，有大量锐角，这暗示出现过拟合现象。相反，随机森林和 Bagging 的边界相当平滑，没有明显的过拟合的迹象。

## 随机森林偏差
![[Pasted image 20241025153041.png]]
![[Pasted image 20241025153059.png]]
![[Pasted image 20241025153205.png]]
## 随机森林的优势和劣势
优势：
- 高预测准确率。在大多数问题上表现优于线性算法，准确率与 Boosting 相当；
- 随机取样导致其对离散值的鲁棒性较好；
- 随机选取子空间导致其对特征缩放及其他单调转换不敏感；
- 不需要精细的参数调整。
- 在具有大量特征和类别的数据集上很高效；
- 既可处理连续值，也可处理离散值；
- 不容易出现过拟合。在实践中，增加树的数量几乎总是能提升总体表现。不过，当树的数量增大到一定值后，学习曲线将趋于平稳；
- 有理论方法可以估计特征的重要性；
- 能够很好地处理数据缺失。即使有大部分的数据缺失现象，仍能保持较好的准确率；
- 支持整个数据集及单棵树样本上的加权分类；
- 决策树底层使用的相似性计算可以用于后续的聚类、离散值检测或兴趣数据表示；
- 以上功能和性质可以扩展到未标注数据，以支持无监督聚类，数据可视化和离散值检测；
- 易于并行化，伸缩性强。
劣势：
- 相比单棵决策树，随机森林的输出更难解释；
- 特征重要性估计没有形式化的 p 值；
- 在数据稀疏时（比如，文本输入、词袋），表现不如线性模型好；
- 和线性回归不同，随机森林无法外推。不过，这样也就不会因为离散值的存在导致极端值的出现；
- 在某些问题上容易过拟合，特别是处理高噪声数据时；
- 处理数量级不同的类别数据时，随机森林偏重数量级较高的数据，因为这能更明显的提高准确率；
- 所得模型较大，需要大容量的 RAM 来支撑。

## 特征重要性
我们常常需要对算法的输出结果做出解释，在不能完全理解算法的情况下，至少希望能找出哪个输入特征对结果的贡献最大。基于随机森林，我们可以相当容易地获取这类信息。![[Pasted image 20241025153452.png]]![[Pasted image 20241025153508.png]]![[Pasted image 20241025153517.png]]
## 梯度提升
 AdaBoost 采用的是一种贪婪的学习方法。在算法学习时，会给数据样本的每个都会附一个权重值，每一个基分类器开始进行分类前，都会根据前一个分类器的分类误差来调节样本的权重值。然后通线性组合的方式把所有的基分类器集成起来得到一个强分类器。整个过程可以描述为下图所示。![[Pasted image 20241025153709.png]]
 ![[Pasted image 20241025153852.png]]
## GBM算法
![[Pasted image 20241025154003.png]]
## 损失函数
### 回归问题
![[Pasted image 20241025154027.png]]
### 分类损失
![[Pasted image 20241025154050.png]]